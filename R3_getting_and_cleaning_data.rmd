---
title: "R3 | Getting and Cleaning Data"
author: "Sean Mussenden"
date: "3/2/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### What We're Doing

In this hourlong session, we're going to show you:

* How to pull flat data (tables or spreadsheets) into R in several different ways.
  * Read in data that comes preloaded as part of a package.
  * Read in a CSV file on your local machine
  * Download a CSV file located somewhere on the Internet.
  * Read in data from an Excel file stored locally.
  * Read in data from a Google Sheet.
  * Read in data from an HTML table from the Internet and coerce it into an R dataframe, using the RVest package.
  
* Introduce you to, but not teach in detail, information about alternate data loading methods, including packages that will help you:
  * Pull data from an SQL database. 
  * Pull data from an API endpoint on the web.
  * Easily pull data from specific APIs (like Twitter or the U.S. Census).
  * Load nested data, like json. 
  
* Do some basic clean tasks once you've pulled it in.
  * Bulk rename dirty column names, using the Janitor package.
  * Convert column types, including dates, numbers and text. 
  * Manipulate columns containing strings to make them usable, and create new columns. 

This class isn't designed to make you an expert in any of these concepts. We're introducing them to you, as a foundation for further learning.  

### How We're Doing It

In the exercises below, we'll work through some examples together.  And then we'll give you a chance to try it out on your own.  

### Getting Started 

First, you'll need to know the location of the GitHub repository for this course. 

It's https://github.com/walinchus/BabyNames. Go ahead and load that into a web browser.   

You should also create a new RProject for this course.  Among other things, it will set your working directory so local files load correctly.  Go to File > New Project > Existing Directory and select the desktop folder for this course.      

Then, create a new R script file (File -> New File -> R Script) and save it in that same folder.

### Loading Libraries

We'll need to load several packages for this class.  They are:

* The [Tidvyerse](https://www.tidyverse.org/) collection of packages. We'll be making extensive use of the readr, dplyr and stringr packages,which all load as part of the tidyverse core.  
* The [rvest](http://rvest.tidyverse.org/) package for web scraping.
* The [janitor](https://github.com/sfirke/janitor) package for data cleaning. 
* The [readxl](https://readxl.tidyverse.org/) package for loading Excel files. 
* The [googlesheets4](https://googlesheets4.tidyverse.org/) package for reading -- and writing -- data stored in a Google Sheet. 
* The [lubridate](https://lubridate.tidyverse.org/) package for working with dates.


```{r}
#install.packages('babynames')
#install.packages('tidyverse')
#install.packages('rvest')
#install.packages('janitor')
#install.packages('readxl')
#install.packages('googlesheets4')
#install.packages('babynames')

# Turn off scientific notation
options(scipen=999)

# Load libraries
library(tidyverse)
library(rvest)
library(janitor)
library(readxl)
library(googlesheets4)
library(babynames)


```

### Loading Local Data

#### Loading preloaded data from a package

If you've taken earlier classes in this sequence, you'll have loaded data of Social Security Administration data on names of babies born across the last ~100 years. It comes built in with the 'babynames' package we loaded. 

Let's load that now and store it as an object called `babynames_package`


```{r}

# Load babynames
babynames_package <- babynames

# Show first 10 records
babynames_package %>%
  head(10)

babynames_package %>%
  filter(str_detect(name,"^A")) %>%
  write_csv("data/R3/babynames_a.csv")

```

#### Loading data from a local flat file

Loading data from a flat file -- like a csv -- stored on your local machine is a very common data loading task.  For that, we'll use the `read_csv()` function that's part of the `readr` package that loads with the `tidyverse`. 

We're going to read in a sample of the babynames data that contains all of the names that start with "A". 

Store it as an object called "babynames_a_csv" (the name doesn't matter, can be anything). The information inside of `read_csv()` is the filepath to the data, which is stored in the data > R3. 

```{r}

babynames_a_csv <- read_csv("data/R3/babynames_a.csv")

```

#### Loading data from a flat file on the internet

We can load that same csv from the internet, instead of doing it locally.  We just need to change the information inside of the `read_csv()` function to pass a URL instead of a location on our computer. The CSV is up on Github.com, here: https://raw.githubusercontent.com/walinchus/BabyNames/master/data/R3/babynames_a.csv

```{r}
babynames_a_url <- read_csv("https://raw.githubusercontent.com/walinchus/BabyNames/master/data/R3/babynames_a.csv")
```


#### Loading data from an excel file

Another data format you'll likely see in the wild: Excel files. They're a little trickier than csvs, because they can contain multiple sheets. 

We can use the `read_xlsx()` function to load data from Excel files. There's an Excel file in the data/R3 folder called babynames_excel.  It has two sheets, one with all of the "A" babynames, followed by one with all of the "B" babynames. 

Let's load that As as a dataframe and store it as an object called babynames_a_excel.

```{r}
babynames_a_excel <- read_xlsx("data/R3/babynames_excel.xlsx")
```

Because "A" is the first sheet, the function loads the "A" sheet. But if we need to get any other sheet, like "B" we'll have to get it by adding another argument to the read_xlsx function. 

Let's load B now and store it as an object called babynames_b_excel. Notice we've included the name of the sheet.

```{r}
babynames_b_excel <- read_xlsx("data/R3/babynames_excel.xlsx", sheet="babynames_b")

```

## Loading data from google sheets

We can also load data directly from Google Sheets, using the `read_sheet()` function from the `googlesheets4` package.  Google Sheets, like Excel files, can also have multiple workbooks. 

I've loaded up a Google Sheet workbook here with two sheets, one for "A" babynames and one for "B" babynames: https://docs.google.com/spreadsheets/d/1GG_RmYPGCNKbLb4x1dkQv1B9NjDBF0kQRtge9Aea4rs.

To read in the sheet, we need to paste in the sheet ID, which is the long sting of numbers and letters at the end of the URL. 

Because the "A" sheet comes first, it automatically loads that sheet.

```{r}
babynames_a_sheet <- read_sheet("1GG_RmYPGCNKbLb4x1dkQv1B9NjDBF0kQRtge9Aea4rs")
```
If we want to load any other sheet, we need to tell it which one to load. 

```{r}
babynames_b_sheet <- read_sheet("1GG_RmYPGCNKbLb4x1dkQv1B9NjDBF0kQRtge9Aea4rs", sheet = "babynames_b")
```

A few notes on the `googlesheets4` package.  

* If you want to access a sheet that isn't public on the web, you'll have to authenticate R studio with Google first.  You can do that by running `gs4_auth()`.
* In addition to loading data from Google Sheets, you can also WRITE data from R Studio to Google sheets.  This is such a great way to share data you've analyzed with other, less tech-savvy members of your team. More on that here: https://googlesheets4.tidyverse.org/index.html.  

## Loading data from an HTML table

On this page is an HTML table showing the top 2020 baby names. https://www.ssa.gov/oact/babynames/

We can use the inspector in our web browser to see this, that it's composed of html tags. Using the `rvest` package we're going to first read in the entire html page using the `read_html()` function.

```{r}

top_2020_baby_names <- read_html("https://www.ssa.gov/oact/babynames/") 

```

Open it up in the environment window.  It's the full HTML of the page.  

Next, we'll use `rvest` to extract the html table we want using `html_table()`

```{r}
top_2020_baby_names <- read_html("https://www.ssa.gov/oact/babynames/") %>%
  html_table()

```

Open it up in the environment window. We've isolated that one html table and turned it into a dataframe, albeit one that is nested in a list.  Let's extract it from the list. 

```{r}
top_2020_baby_names <- top_2020_baby_names[[1]]

```

Never web scraped before? Now you have! A warning that web scraping does get more challenging from here. 

### Data Cleaning 


```{r}
babynames_package %>%
  filter(str_detect(name,"^A")) %>%
  write_csv("data/R3/babynames_a.csv")

babynames_a <- babynames %>%
  filter(str_detect(name,"^A")) 

babynames_b <- babynames %>%
  filter(str_detect(name, "^B"))
 
list_of_dfs <- list("babynames_a" = babynames_a, "babynames_b" = babynames_b)

babynames_excel <- write_xlsx(list_of_dfs,path="data/R3/babynames_excel.xlsx")

babynames_excel_sample

babynames_a_sample <- babynames %>%
  filter(str_detect(name,"^A")) %>%
  head(1000)

babynames_b_sample <- babynames %>%
  filter(str_detect(name, "^B")) %>%
  head(1000)
list_of_dfs <- list("babynames_a" = babynames_a_sample, "babynames_b" = babynames_b_sample)

babynames_excel_sample <- write_xlsx(list_of_dfs,path="data/R3/babynames_excel_sample.xlsx")

```


Going forward, we're going to work with the grandparents_local file. 

Let's load it again and view it. 

```{r}
grandparents_local <- read_csv("data/grandparentsR1.csv")
grandparents_local
```

It's got some funky CamelCase column names, a mix of lower and uppercase letters, and some underscores. Let's standarize that, using a function called clean_names() from the [janitor package](https://github.com/sfirke/janitor)

```{r}
grandparents_local <- grandparents_local %>%
  clean_names()
```

Let's view it again. 

```{r}
grandparents_local
```

Janitor has a lot of other data cleaning functionality that's worth knowing about, but that we won't have a lot of time to cover, including:
* It can identify duplicate rows
* It can identify and remove blank rows

This data set is relatively clean, but there's still some work we can do to it.

Let's suppose we want to split our geography column into two columns, one with just the parish and one with just the state. 

We can use the [separate() function](https://tidyr.tidyverse.org/reference/separate.html) here. We're telling it to split the column geography into two columns (parish and state), and to split them where you see a column.

```{r}
grandparents_local <- grandparents_local %>%
  separate(geography, into=c("parish_name","state"), sep=",")

grandparents_local
```

Okay, good.  But that caused a problem.  The state column now has a tiny bit of white space at the front.We can see it when we highlight the column, using view.  

Let's get rid of that, using [str_trim](https://stringr.tidyverse.org/reference/str_trim.html) from the [stringr package](https://stringr.tidyverse.org/index.html)

```{r}
grandparents_local <- grandparents_local %>%
  mutate(state = str_trim(state))

grandparents_local
```

Okay, good, but let's suppose we wanted to join our parish data with another data set from the census, to explore relationships between demographic factors like race and poverty and grandchildren living with grandparents.  

And let's suppose that the Census had parish names all lowercase, with no caps.

```{r}
grandparents_local <- grandparents_local %>%
  mutate(parish_name = tolower(parish_name))

grandparents_local
```

Or, even worse, let's imagine that the Census data we want to join it with doesn't have the word "parish" in it. We can use [str_remove()](https://stringr.tidyverse.org/reference/str_remove.html) from stringr. 

```{r}
grandparents_local <- grandparents_local %>%
  mutate(parish_name = str_remove(parish_name, "parish"))

grandparents_local
```

It's got that whitespace again, so let's trim it. Go ahead and do that on your own. 

```{r}
grandparents_local <- grandparents_local %>%
  mutate(parish_name = str_trim(parish_name))

grandparents_local
```

Lastly, we're going to change the column type of id2.  It's stored as a "double", a number format.  But it's a FIPS code, and we'd never do math on it.  It makes more sense to store it as a character.  

```{r}
grandparents_local <- grandparents_local %>%
  mutate(id2 = as.character(id2))
grandparents_local
```

In addition to as.character, there's also as.numeric and other functions for changing column types.

Lastly, we'll learn how to split a string based on space. id2 is a FIPS code.  Two digit state, 3 digit county. 

We can use [str_sub()](https://stringr.tidyverse.org/reference/str_sub.html) from the stringr package to take the first two digits from the left and store them in a new column called state_code.

```{r}

grandparents_local <- grandparents_local %>%
  mutate(state_code = str_sub(id2, start=1L, end=2L))
```

On your own, create a new column for the county code. You want to get the last three digits.   

```{r}
grandparents_local <- grandparents_local %>%
  mutate(county_code = str_sub(id2, start=3L, end=5L))

```

This is only scratching the surface for data cleaning. There's a ton more you can do. 

I highly recomment checking out the documentation for:
*[Janitor](https://github.com/sfirke/janitor)
*[Stringr](https://stringr.tidyverse.org/index.html)

-30-
